{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file contains a set of functions to implement using PCA.\n",
    "All of them take at least a dataframe df as argument. To test your functions\n",
    "locally, we recommend using the wine dataset that you can load from sklearn by\n",
    "importing sklearn.datasets.load_wine\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulated_variance(df, scale):\n",
    "    \"\"\"Apply PCA on a DataFrame and return a new DataFrame containing\n",
    "    the cumulated explained variance from using the first component only,\n",
    "    up to using all components together. Values should be expressed as\n",
    "    a percentage of the total variance explained.\n",
    "\n",
    "    The DataFrame will have one row and each column should correspond to a\n",
    "    principal component.\n",
    "\n",
    "    Example:\n",
    "             PC1        PC2        PC3        PC4    PC5\n",
    "    0  36.198848  55.406338  66.529969  73.598999  100.0\n",
    "\n",
    "    If scale is True, you should standardise the data first\n",
    "    Tip: use the StandardScaler from sklearn\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param scale: boolean, whether to scale or not\n",
    "    :return: a new DataFrame with cumulated variance in percent\n",
    "    \"\"\"\n",
    "\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        df = scaler.fit_transform(df)\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(df)\n",
    "\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_).reshape(1, -1)\n",
    "    return pd.DataFrame(\n",
    "        100 * cumsum,\n",
    "        columns=[\"PC{}\".format(n + 1) for n in range(df.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_of_first_two(df, scale):\n",
    "    \"\"\"Apply PCA on a given DataFrame df and return a new DataFrame\n",
    "    containing the coordinates of the two first principal components\n",
    "    expressed in the original basis (with the original columns).\n",
    "\n",
    "    Example:\n",
    "    if the original DataFrame was:\n",
    "\n",
    "          A    B\n",
    "    0   1.3  1.2\n",
    "    1    27  2.1\n",
    "    2   3.3  6.8\n",
    "    3   5.1  3.2\n",
    "\n",
    "    we want the components PC1 and PC2 expressed as a linear combination\n",
    "    of A and B, presented in a table as:\n",
    "\n",
    "              A    B\n",
    "    PC1     0.1  1.1\n",
    "    PC2       3    1\n",
    "\n",
    "    If scale is True, you should standardise the data first\n",
    "    Tip: use the StandardScaler from sklearn\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param scale: boolean, whether to scale or not\n",
    "    :return: a new DataFrame with coordinates of PC1 and PC2\n",
    "    \"\"\"\n",
    "\n",
    "    cols = df.columns\n",
    "\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        df = scaler.fit_transform(df)\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(df)\n",
    "\n",
    "    first_two = pca.components_[:2]\n",
    "    return pd.DataFrame(first_two, columns=cols, index=[\"PC1\", \"PC2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_two(df, scale):\n",
    "    \"\"\"Apply PCA on a given DataFrame df and use it to determine the\n",
    "    'most important' features in your dataset. To do so we will focus\n",
    "    on the principal component that exhibits the highest explained\n",
    "    variance (that's PC1).\n",
    "\n",
    "    PC1 can be expressed as a vector in the original basis (our original\n",
    "    columns). Here we want to return the names of the two features that\n",
    "    have the highest absolute weight in PC1.\n",
    "\n",
    "    Example:\n",
    "        if the original DataFrame was:\n",
    "\n",
    "          A    B    C\n",
    "    0   1.3  1.2  0.1\n",
    "    1    27  2.1  1.2\n",
    "    2   3.3  6.8  3.4\n",
    "    3   5.1  3.2  4.5\n",
    "\n",
    "    and PC1 can be written as [.01, .9, .2] in [A, B, C].\n",
    "\n",
    "    Then you should return B, C as the two most important features.\n",
    "\n",
    "    If scale is True, you should standardise the data first\n",
    "    Tip: use the StandardScaler from sklearn\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param scale: boolean, whether to scale or not\n",
    "    :return: names of the two most important features as a tuple\n",
    "    \"\"\"\n",
    "\n",
    "    cols = df.columns\n",
    "\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        df = scaler.fit_transform(df)\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(df)\n",
    "\n",
    "    pc1_abs = np.abs(pca.components_[0])\n",
    "    first, second = np.argsort(pc1_abs)[::-1][:2]\n",
    "    return cols[first], cols[second]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_in_n_dimensions(df, point_a, point_b, n, scale):\n",
    "    \"\"\"Write a function that applies PCA on a given DataFrame df in order to learn\n",
    "    a new subspace of dimension n.\n",
    "\n",
    "    Project the two points point_a and point_b into that n dimensions space,\n",
    "    compute the Euclidean distance between the points in that space and return it.\n",
    "\n",
    "    Example:\n",
    "        if the original DataFrame was:\n",
    "\n",
    "          A    B    C\n",
    "    0   1.3  1.2  0.1\n",
    "    1    27  2.1  1.2\n",
    "    2   3.3  6.8  3.4\n",
    "    3   5.1  3.2  4.5\n",
    "\n",
    "    and n = 2, you can learn a new subspace with two columns [PC1, PC2].\n",
    "\n",
    "    Then given two points:\n",
    "\n",
    "    point_a = [1, 2, 3]\n",
    "    point_b = [2, 3, 4]\n",
    "    expressed in [A, B, C]\n",
    "\n",
    "    Project them into [PC1, PC2] and return the Euclidean distance between the\n",
    "    points in that space.\n",
    "\n",
    "    If scale is True, you should standardise the data first\n",
    "    Tip: use the StandardScaler from sklearn\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param point_a: a numpy vector expressed in the same basis as df\n",
    "    :param point_b: a numpy vector expressed in the same basis as df\n",
    "    :param n: number of dimensions of the new space\n",
    "    :param scale: whether to scale data or not\n",
    "    :return: distance between points in the subspace\n",
    "    \"\"\"\n",
    "\n",
    "    point_a = point_a.reshape(1, -1)\n",
    "    point_b = point_b.reshape(1, -1)\n",
    "\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        df = scaler.fit_transform(df)\n",
    "        point_a = scaler.transform(point_a)\n",
    "        point_b = scaler.transform(point_b)\n",
    "\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(df)\n",
    "\n",
    "    return np.sqrt(np.sum((pca.transform(point_a) - pca.transform(point_b))**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_pca(df, n, scale):\n",
    "    \"\"\"Apply PCA on a given DataFrame df and project all the data\n",
    "    on the first principal component.\n",
    "\n",
    "    With all the points projected in a one-dimension space, find outliers\n",
    "    by looking for points that lie at more than n standard deviations from the mean.\n",
    "\n",
    "    You should return a new dataframe containing all the rows of the original dataset\n",
    "    that have been found to be outliers when projected.\n",
    "\n",
    "    Example:\n",
    "        if the original DataFrame was:\n",
    "\n",
    "          A    B    C\n",
    "    0   1.3  1.2  0.1\n",
    "    1    27  2.1  1.2\n",
    "    2   3.3  6.8  3.4\n",
    "    3   5.1  3.2  4.5\n",
    "\n",
    "    Once projected on PC1 it will be:\n",
    "        PC1\n",
    "    0     1\n",
    "    1   1.1\n",
    "    2   2.1\n",
    "    3   100\n",
    "\n",
    "    Compute the mean of this one dimensional dataset and find all rows that lie at more\n",
    "    than n standard deviations from it. Here only the row 3 is an outlier.\n",
    "\n",
    "    So you should return:\n",
    "          A    B    C\n",
    "    3   5.1  3.2  4.5\n",
    "\n",
    "    If scale is True, you should standardise the data first\n",
    "    Tip: use the StandardScaler from sklearn\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param n: number of standard deviations from the mean to be considered outlier\n",
    "    :param scale: whether to scale data or not\n",
    "    :return: pandas DataFrame containing outliers only\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        df_copy = scaler.fit_transform(df)\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    df_pca = pca.fit_transform(df_copy).squeeze()\n",
    "    mean_ = df_pca.mean()\n",
    "    std = df_pca.std()\n",
    "\n",
    "    msk_high = df_pca > mean_ + n * std\n",
    "    msk_low = df_pca < mean_ - n * std\n",
    "\n",
    "    return df.loc[msk_high | msk_low]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
